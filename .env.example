# Natural language parsing — pick ONE provider (or leave all blank for regex fallback)
# Priority: OLLAMA_URL > ANTHROPIC_API_KEY > OPENAI_API_KEY > regex

# Local LLM via Ollama (recommended — free, private, fast)
# Install: https://ollama.ai  |  Pull a model: ollama pull llama3.2
# OLLAMA_URL=http://host.docker.internal:11434
# OLLAMA_MODEL=llama3.2

# Anthropic API
# ANTHROPIC_API_KEY=sk-ant-...

# OpenAI API
# OPENAI_API_KEY=sk-...

# Optional: override the model name (for Anthropic/OpenAI providers)
# LLM_MODEL=claude-sonnet-4-20250514

# Freezer dimensions in inches (W x D x H) — defaults to 33 x 20 x 34
# VITE_FREEZER_W=33
# VITE_FREEZER_D=20
# VITE_FREEZER_H=34

# Tailscale: add your hostname so Vite allows proxied requests
# VITE_ALLOWED_HOSTS=your-machine.your-tailnet.ts.net
